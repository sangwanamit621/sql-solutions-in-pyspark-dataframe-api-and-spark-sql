{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ecdfea-03b9-49aa-80ec-b1d34e6f02a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DateType\n",
    "from pyspark.sql.functions import col,sum,count,when,date_format\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a25cd43-b78e-4c9c-88c6-742cffc3ea55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+----------+\n| id|country|   state|amount|trans_date|\n+---+-------+--------+------+----------+\n|121|     US|approved|  1000|2018-12-18|\n|122|     US|declined|  2000|2018-12-19|\n|123|     US|approved|  2000|2019-01-01|\n|124|     DE|approved|  2000|2019-01-07|\n+---+-------+--------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\",IntegerType(),False),\n",
    "    StructField(\"country\",StringType(),False),\n",
    "    StructField(\"state\",StringType(),False),\n",
    "    StructField(\"amount\",IntegerType(),False),\n",
    "    StructField(\"trans_date\",DateType(),False)\n",
    "])\n",
    "data = [\n",
    "( 121  , 'US'      , 'approved' , 1000   , datetime(2018,12,18)) ,\n",
    "( 122  , 'US'      , 'declined' , 2000   , datetime(2018,12,19)) ,\n",
    "( 123  , 'US'      , 'approved' , 2000   , datetime(2019, 1, 1)) ,\n",
    "( 124  , 'DE'      , 'approved' , 2000   , datetime(2019, 1, 7)) \n",
    "]\n",
    "transactions = spark.createDataFrame(data,schema)\n",
    "transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e434810-0d6a-496d-a2a2-f9d32cb33570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------+-----------+------------------+------------------+---------------------+\n|date_format(trans_date, y-M)|country|trans_count|trans_total_amount|trans_total_amount|approved_total_amount|\n+----------------------------+-------+-----------+------------------+------------------+---------------------+\n|                     2018-12|     US|          2|                 1|              3000|                 1000|\n|                      2019-1|     US|          1|                 1|              2000|                 2000|\n|                      2019-1|     DE|          1|                 1|              2000|                 2000|\n+----------------------------+-------+-----------+------------------+------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "# Return the result table in any order.\n",
    "\n",
    "transactions.groupBy(date_format(col(\"trans_date\"),'y-M'),'country')\\\n",
    "    .agg(\\\n",
    "        count(\"country\").alias(\"trans_count\"), \\\n",
    "        count( when(col(\"state\")==\"approved\",1).otherwise(None)).alias(\"trans_total_amount\"), \\\n",
    "        sum(col(\"amount\")).alias(\"trans_total_amount\"),\\\n",
    "        sum( when(col(\"state\")==\"approved\",col(\"amount\")).otherwise(0) ).alias(\"approved_total_amount\")\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac039fa-a833-4244-b383-0985f42c4d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+--------------+------------------+---------------------+\n|  month|country|trans_count|approved_count|trans_total_amount|approved_total_amount|\n+-------+-------+-----------+--------------+------------------+---------------------+\n|2018-12|     US|          2|             1|              3000|                 1000|\n|2019-01|     US|          1|             1|              2000|                 2000|\n|2019-01|     DE|          1|             1|              2000|                 2000|\n+-------+-------+-----------+--------------+------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "transactions.createOrReplaceTempView(\"trans\")\n",
    "spark.sql(\"\"\"select date_format(trans_date,'y-MM') as `month`,country,\n",
    "                    count(*) as trans_count, \n",
    "                    count(case when state='approved' then amount else null end) approved_count,\n",
    "                    sum(amount) trans_total_amount,\n",
    "                    sum(case when state='approved' then amount else 0 end) approved_total_amount                 \n",
    "            from trans group by 1,2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a488962c-f0f6-48fb-865b-3c8d7b2fcedf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5fc9a7-6ba6-4e57-89f4-c17e0a27f8fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1193. Monthly Transactions I",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
