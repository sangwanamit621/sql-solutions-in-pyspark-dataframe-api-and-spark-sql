{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b79b394-5a62-4905-b144-e87873ab0f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,DateType,IntegerType\n",
    "from pyspark.sql.functions import col, avg,sum,round\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda0607c-0aae-400d-aede-2ebc4f24bf5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+\n|product_id|start_date|  end_date|price|\n+----------+----------+----------+-----+\n|         1|2019-02-17|2019-02-28|    5|\n|         1|2019-03-01|2019-03-22|   20|\n|         2|2019-02-01|2019-02-20|   15|\n|         2|2019-02-21|2019-03-31|   30|\n+----------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\",IntegerType(),False),\n",
    "    StructField(\"start_date\",DateType(),False),\n",
    "    StructField(\"end_date\",DateType(),False),\n",
    "    StructField(\"price\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    ( 1          , datetime(2019,2,17) , datetime(2019,2,28) , 5      ),\n",
    "    ( 1          , datetime(2019,3,1) , datetime(2019,3,22) , 20      ),\n",
    "    ( 2          , datetime(2019,2,1) , datetime(2019,2,20) , 15      ),\n",
    "    ( 2          , datetime(2019,2,21) , datetime(2019,3,31) , 30     )\n",
    "]\n",
    "\n",
    "prices = spark.createDataFrame(data,schema)\n",
    "prices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9e55a1-1d0b-4289-ab66-584eb5f11bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+\n|product_id|purchase_date|units|\n+----------+-------------+-----+\n|         1|   2019-02-25|  100|\n|         1|   2019-03-01|   15|\n|         2|   2019-02-10|  200|\n|         2|   2019-03-22|   30|\n+----------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\",IntegerType(),False),\n",
    "    StructField(\"purchase_date\",DateType(),False),\n",
    "    StructField(\"units\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    ( 1          , datetime(2019,2,25)    , 100   ),\n",
    "    ( 1          , datetime(2019,3,1 )    , 15    ),\n",
    "    ( 2          , datetime(2019,2,10)    , 200   ),\n",
    "    ( 2          , datetime(2019,3,22)    , 30    )\n",
    "]\n",
    "\n",
    "sale = spark.createDataFrame(data,schema)\n",
    "sale.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d92f84-53ef-4a4e-93b8-82b433dc9d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|product_id|average_price|\n+----------+-------------+\n|         1|         6.96|\n|         2|        16.96|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "# Return the result table in any order. There can be products who were never sold\n",
    "sale.join(prices,((sale.hint(\"range_join\",3).product_id==prices.product_id) & (sale.purchase_date.between(prices.start_date,prices.end_date)) ),'right')\\\n",
    "    .withColumn(\"sale_amount\",sale.units*prices.price).groupBy(sale.product_id).agg(round(sum(\"sale_amount\")/sum(\"units\"),2).alias(\"average_price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd62101-4da5-4c69-b8e2-e61f3372061d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|product_id|average_price|\n+----------+-------------+\n|         1|         6.96|\n|         2|        16.96|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "sale.createOrReplaceTempView(\"s\")\n",
    "prices.createOrReplaceTempView(\"p\")\n",
    "spark.sql(\"\"\"select p.product_id, coalesce(round(sum(p.price*s.units)/sum(s.units),2),0) as average_price \n",
    "                from  s right join  \n",
    "                p \n",
    "                on s.product_id=p.product_id and s.purchase_date between p.start_date and p.end_date \n",
    "                group by p.product_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e017086-8881-4746-a159-8bd5b8e4f9e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b9a771-775d-4b5a-9946-cec2413a542e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1251. Average Selling Price",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
