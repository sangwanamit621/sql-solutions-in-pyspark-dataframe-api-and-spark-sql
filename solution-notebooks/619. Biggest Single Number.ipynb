{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3515e0c-9860-4f8f-8ba3-cf8139c13426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "from pyspark.sql.functions import count,max,col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c59668-6057-4487-b826-506be0c80619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  8|\n|  8|\n|  3|\n|  3|\n|  1|\n|  1|\n|  6|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"num\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "# Note: DataFrame expects data to be list of list or list of tuple so we are using (value,) so that it can become a tuple else we will get error -> StructType can not accept object 8 in type <class 'int'>\n",
    "data = [\n",
    "    (8,), \n",
    "    (8,),  \n",
    "    (3,),  \n",
    "    (3,), \n",
    "    (1,), \n",
    "    (1,), \n",
    "    (6,), \n",
    "    (6,)\n",
    "]\n",
    "\n",
    "nums = spark.createDataFrame(data,schema)\n",
    "nums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926f093f-05aa-46ff-939b-326ce7de9ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n| num|\n+----+\n|null|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# A single number is a number that appeared only once in the MyNumbers table. Find the largest single number. If there is no single number, report null.\n",
    "\n",
    "nums.groupBy(\"num\").agg(count(\"num\").alias(\"count\")).filter(col(\"count\")==1).select(\"num\").agg(max(\"num\").alias(\"num\")).show() # case when there is no single largest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a91f18-f652-42e6-923c-570f6af801d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  8|\n|  8|\n|  3|\n|  3|\n|  1|\n|  2|\n|  5|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (8,), \n",
    "    (8,),  \n",
    "    (3,),  \n",
    "    (3,), \n",
    "    (1,), \n",
    "    (2,), \n",
    "    (5,), \n",
    "    (6,)\n",
    "]\n",
    "\n",
    "nums1 = spark.createDataFrame(data,schema)\n",
    "nums1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f5ecf2-11cc-4b22-b20e-6946fc54d9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "nums1.groupBy(\"num\").agg(count(\"num\").alias(\"count\")).where(col(\"count\")==1).select(\"num\").agg(max(\"num\").alias(\"num\")).show() # case when single largest number is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d099bd7d-5e42-4a84-9b61-961e586465c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "nums1.createOrReplaceTempView(\"nums\")\n",
    "\n",
    "spark.sql(\"select max(num) as num from (select num from nums group by num having count(*)=1) nums1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a138f99f-c1a2-4ca0-b41d-191452a9498e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f172c7b0-2873-4c29-b0eb-5dc8a2329471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "619. Biggest Single Number",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
