{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6902627-96f9-4c0f-9d3b-41eb4f0dccc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,DateType\n",
    "from pyspark.sql.functions import col,round, min,sum,count_distinct,when\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bf32476-7710-4509-8082-20a46b28c87c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n+-----------+-----------+----------+---------------------------+\n|          1|          1|2019-08-01|                 2019-08-02|\n|          2|          2|2019-08-02|                 2019-08-02|\n|          3|          1|2019-08-11|                 2019-08-12|\n|          4|          3|2019-08-24|                 2019-08-24|\n|          5|          3|2019-08-21|                 2019-08-22|\n|          6|          2|2019-08-11|                 2019-08-13|\n|          7|          4|2019-08-09|                 2019-08-09|\n+-----------+-----------+----------+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"delivery_id\",IntegerType(),False),\n",
    "    StructField(\"customer_id\",IntegerType(),False),\n",
    "    StructField(\"order_date\",DateType(),False),\n",
    "    StructField(\"customer_pref_delivery_date\",DateType(),False)\n",
    "\n",
    "])\n",
    "data = [\n",
    "( 1           , 1           , datetime(2019,8, 1) , datetime(2019,8, 2))                  ,\n",
    "( 2           , 2           , datetime(2019,8, 2) , datetime(2019,8, 2))                  ,\n",
    "( 3           , 1           , datetime(2019,8,11) , datetime(2019,8,12))                  ,\n",
    "( 4           , 3           , datetime(2019,8,24) , datetime(2019,8,24))                  ,\n",
    "( 5           , 3           , datetime(2019,8,21) , datetime(2019,8,22))                  ,\n",
    "( 6           , 2           , datetime(2019,8,11) , datetime(2019,8,13))                  ,\n",
    "( 7           , 4           , datetime(2019,8, 9) , datetime(2019,8, 9))  \n",
    "]\n",
    "delivery = spark.createDataFrame(data,schema)\n",
    "delivery.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af6b01b-9005-465b-a6b5-07fa2040003a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|immediate_percentage|\n+--------------------+\n|                50.0|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "# The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "# Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "window_spec = Window.partitionBy(\"customer_id\")\n",
    "total_entities = delivery.select(\"customer_id\").distinct().count()\n",
    "delivery.withColumn(\"first_order\",min(\"order_date\").over(window_spec)).agg(round(100*sum(when(col(\"first_order\")==col(\"customer_pref_delivery_date\"),1).otherwise(0))/total_entities,2).alias(\"immediate_percentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8beacdd8-bf21-4127-a083-b194a0d75fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|immediate_percentage|\n+--------------------+\n|                50.0|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "delivery.createOrReplaceTempView(\"del\")\n",
    "spark.sql(\"\"\"with cte as \n",
    "                    (\n",
    "                        select customer_id,customer_pref_delivery_date, min(order_date) over(partition by customer_id) as first_order from del\n",
    "                    )\n",
    "            select round(100*sum(case when first_order=customer_pref_delivery_date then 1 else 0 end)/count(distinct customer_id),2) immediate_percentage from cte\n",
    "                    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314dda29-1469-493b-8a19-723da6810dc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828efc90-f28f-44e3-89cc-760322c11b44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1174. Immediate Food Delivery II",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
