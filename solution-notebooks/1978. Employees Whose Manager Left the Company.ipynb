{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557eebb3-d29c-4e57-8b8e-ef72adeb672f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e2ceb4-d69b-4911-b700-18412ace7d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+\n|employee_id|     name|manager_id|salary|\n+-----------+---------+----------+------+\n|          3|     Mila|         9| 60301|\n|         12|Antonella|      null| 31000|\n|         13|    Emery|      null| 67084|\n|          1|    Kalel|        11| 21241|\n|          9|  Mikaela|      null| 50937|\n|         11|   Joziah|         6| 28485|\n|          8|     John|         5| 29485|\n+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"employee_id\",IntegerType(),False),\n",
    "    StructField(\"name\",StringType(),False),\n",
    "    StructField(\"manager_id\",IntegerType(),True),\n",
    "    StructField(\"salary\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "( 3           , \"Mila\"      , 9          , 60301 ) ,\n",
    "( 12          , \"Antonella\" , None       , 31000 ) ,\n",
    "( 13          , \"Emery\"     , None       , 67084 ) ,\n",
    "( 1           , \"Kalel\"     , 11         , 21241 ) ,\n",
    "( 9           , \"Mikaela\"   , None       , 50937 ) ,\n",
    "( 11          , \"Joziah\"    , 6          , 28485 ) ,\n",
    "( 8           , \"John\"      , 5          , 29485 )\n",
    "]\n",
    "emp = spark.createDataFrame(data,schema)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83c4080-749e-4a57-b2c5-d5676da3abe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          8|\n|         11|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the IDs of the employees whose salary is strictly less than $30000 and whose manager left the company. When a manager leaves the company, their information is deleted from the Employees table, \n",
    "# but the reports still have their manager_id set to the manager that left. Return the result table ordered by employee_id.\n",
    "\n",
    "# Note: In pyspark DataFrame API, for leftanti join we can use words: anti, leftanti and left_anti \n",
    "# but for right anti join there is nothing so we have to change the dataframes order and keep the dataframe whose anti values we want to find on left side whereas in spark sql we got both left and right anti joins\n",
    "\n",
    "# Note: In left anti join and left semi join, we get columns of only left dataframe/table\n",
    "emp.alias(\"e\").filter(col(\"salary\")<30000).join(emp.alias(\"m\"),col(\"e.manager_id\")==col(\"m.employee_id\"),\"leftanti\").select(\"e.employee_id\").orderBy(\"employee_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b65904-b08c-47dd-a5f4-49fc109b9175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          8|\n|         11|\n+-----------+\n\nUsing left anti join\n+-----------+\n|employee_id|\n+-----------+\n|          8|\n|         11|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.createOrReplaceTempView(\"emp\")\n",
    "# without join\n",
    "spark.sql(\"select employee_id from emp where salary<30000 and manager_id is not null and manager_id not in (select employee_id from emp) order by 1\").show()\n",
    "\n",
    "# with anti join\n",
    "print(\"Using left anti join\")\n",
    "spark.sql(\"select e.employee_id from emp e left anti join emp m on e.manager_id=m.employee_id where e.salary<30000 order by 1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38adb07-fbc1-477d-b074-5dcb6cda3a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19aa8fa4-3d04-4f42-9268-fb907dff0d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1978. Employees Whose Manager Left the Company",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
