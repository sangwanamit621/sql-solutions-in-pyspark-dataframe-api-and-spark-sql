{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de919854-05ea-4680-a299-355dd8252fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\n",
    "from pyspark.sql.functions import col,year,count\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41dd55c6-c2fc-4828-b68d-bac04f153a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n|user_id| join_date|favorite_brand|\n+-------+----------+--------------+\n|      1|2018-01-01|       Lenovo |\n|      2|2018-02-09|       Samsung|\n|      3|2018-01-19|            LG|\n|      4|2018-05-21|            HP|\n+-------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\",IntegerType(),False),\n",
    "    StructField(\"join_date\",DateType(),False),\n",
    "    StructField(\"favorite_brand\",StringType(),False)\n",
    "])\n",
    "data = [\n",
    "    ( 1       , datetime(2018,1, 1) , 'Lenovo '   )     ,\n",
    "    ( 2       , datetime(2018,2, 9) , 'Samsung'   )     ,\n",
    "    ( 3       , datetime(2018,1,19) , 'LG'        )     ,\n",
    "    ( 4       , datetime(2018,5,21) , 'HP'        )     \n",
    "]\n",
    "users = spark.createDataFrame(data,schema)\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8e0f15-6b25-4120-b1a2-5a1856e421ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------+---------+\n|order_id|order_date|item_id|buyer_id|seller_id|\n+--------+----------+-------+--------+---------+\n|       1|2019-08-01|      4|       1|        2|\n|       2|2018-08-02|      2|       1|        3|\n|       3|2019-08-03|      3|       2|        3|\n|       4|2018-08-04|      1|       4|        2|\n|       5|2018-08-04|      1|       3|        4|\n|       6|2019-08-05|      2|       2|        4|\n+--------+----------+-------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"order_id\",IntegerType(),False),\n",
    "    StructField(\"order_date\",DateType(),False),\n",
    "    StructField(\"item_id\",IntegerType(),False),\n",
    "    StructField(\"buyer_id\",IntegerType(),False),\n",
    "    StructField(\"seller_id\",IntegerType(),False)\n",
    "])\n",
    "data = [\n",
    "    ( 1        , datetime(2019,8,1) , 4       , 1        , 2 )        ,\n",
    "    ( 2        , datetime(2018,8,2) , 2       , 1        , 3 )        ,\n",
    "    ( 3        , datetime(2019,8,3) , 3       , 2        , 3 )        ,\n",
    "    ( 4        , datetime(2018,8,4) , 1       , 4        , 2 )        ,\n",
    "    ( 5        , datetime(2018,8,4) , 1       , 3        , 4 )        ,\n",
    "    ( 6        , datetime(2019,8,5) , 2       , 2        , 4 )        \n",
    "]\n",
    "orders = spark.createDataFrame(data,schema)\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddce383-2efc-4711-8c01-1fdb3f1fabea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|item_id|item_brand|\n+-------+----------+\n|      1|   Samsung|\n|      2|    Lenovo|\n|      3|        LG|\n|      4|        HP|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"item_id\",IntegerType(),False),\n",
    "    StructField(\"item_brand\",StringType(),False)\n",
    "])\n",
    "data = [\n",
    "    ( 1       , 'Samsung'   ) ,\n",
    "    ( 2       , 'Lenovo'    ) ,\n",
    "    ( 3       , 'LG'        ) ,\n",
    "    ( 4       , 'HP'        ) \n",
    "]\n",
    "items = spark.createDataFrame(data,schema)\n",
    "items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd880fc-a84a-4eeb-8ef1-71c79af4c71d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n|buyer_id| join_date|orders_in_2019|\n+--------+----------+--------------+\n|       1|2018-01-01|             1|\n|       2|2018-02-09|             2|\n|       3|2018-01-19|             0|\n|       4|2018-05-21|             0|\n+--------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find for each user, the join date and the number of orders they made as a buyer in 2019. Return the result table in any order.\n",
    "\n",
    "orders.filter(year(col(\"order_date\"))==2019)\\\n",
    "    .groupBy(\"buyer_id\").agg(count(\"buyer_id\").alias(\"orders_in_2019\"))\\\n",
    "    .join(users,orders.buyer_id==users.user_id,'right')\\\n",
    "    .select(col(\"user_id\").alias(\"buyer_id\"),\"join_date\",\"orders_in_2019\").fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e067c109-3314-48b9-9d74-52df7ff9d396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n|buyer_id| join_date|orders_in_2019|\n+--------+----------+--------------+\n|       1|2018-01-01|             1|\n|       2|2018-02-09|             2|\n|       3|2018-01-19|             0|\n|       4|2018-05-21|             0|\n+--------+----------+--------------+\n\n+--------+----------+--------------+\n|buyer_id| join_date|orders_in_2019|\n+--------+----------+--------------+\n|       1|2018-01-01|             1|\n|       2|2018-02-09|             2|\n|       3|2018-01-19|             0|\n|       4|2018-05-21|             0|\n+--------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "orders.createOrReplaceTempView(\"o\")\n",
    "users.createOrReplaceTempView(\"u\")\n",
    "\n",
    "# With cte\n",
    "spark.sql(\"\"\"with cte as\n",
    "          (select buyer_id, count(*) as orders_in_2019 from o where year(order_date)=2019 group by 1) \n",
    "          select u.user_id buyer_id,u.join_date,ifnull(c.orders_in_2019,0) orders_in_2019 from cte c right join u on u.user_id=c.buyer_id\"\"\").show()\n",
    "\n",
    "# Without cte\n",
    "spark.sql(\"\"\"\n",
    "          select u.user_id buyer_id,min(u.join_date) join_date ,count(o.buyer_id) orders_in_2019 from o right join u on u.user_id=o.buyer_id and year(o.order_date)=2019 group by 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73a1f46-168d-49b6-b67c-c65aeefcd56b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a308d4-c605-4db8-b4b5-1eee3c7a07ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1158. Market Analysis I",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
