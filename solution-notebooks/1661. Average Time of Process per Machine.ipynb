{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b07266-0724-4ff1-a2e7-f716356b243e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "from pyspark.sql.functions import col,lag,avg,round,abs\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17806690-ccf2-4cf1-ab8c-80cd02db9c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n|machine_id|process_id|activity_type|timestamp|\n+----------+----------+-------------+---------+\n|         0|         0|        start|    0.712|\n|         0|         0|          end|     1.52|\n|         0|         1|        start|     3.14|\n|         0|         1|          end|     4.12|\n|         1|         0|        start|     0.55|\n|         1|         0|          end|     1.55|\n|         1|         1|        start|     0.43|\n|         1|         1|          end|     1.42|\n|         2|         0|        start|      4.1|\n|         2|         0|          end|    4.512|\n|         2|         1|        start|      2.5|\n|         2|         1|          end|      5.0|\n+----------+----------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "schema  = StructType([\n",
    "    StructField(\"machine_id\",IntegerType(),False),\n",
    "    StructField(\"process_id\",IntegerType(),False),\n",
    "    StructField(\"activity_type\",StringType(),False),\n",
    "    StructField(\"timestamp\",FloatType(),False)\n",
    "])\n",
    "data = [\n",
    "    ( 0          , 0          , \"start\"         , 0.712 )    ,\n",
    "    ( 0          , 0          , \"end\"           , 1.520 )    ,\n",
    "    ( 0          , 1          , \"start\"         , 3.140 )    ,\n",
    "    ( 0          , 1          , \"end\"           , 4.120 )    ,\n",
    "    ( 1          , 0          , \"start\"         , 0.550 )    ,\n",
    "    ( 1          , 0          , \"end\"           , 1.550 )    ,\n",
    "    ( 1          , 1          , \"start\"         , 0.430 )    ,\n",
    "    ( 1          , 1          , \"end\"           , 1.420 )    ,\n",
    "    ( 2          , 0          , \"start\"         , 4.100 )    ,\n",
    "    ( 2          , 0          , \"end\"           , 4.512 )    ,\n",
    "    ( 2          , 1          , \"start\"         , 2.500 )    ,\n",
    "    ( 2          , 1          , \"end\"           , 5.000 )   \n",
    "]\n",
    "\n",
    "activity = spark.createDataFrame(data,schema) \n",
    "activity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257ac126-ae00-4eda-ad3c-874b6f4d393d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         1|          0.995|\n|         2|          1.456|\n|         0|          0.894|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "# The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "# The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "# Return the result table in any order.\n",
    "\n",
    "# Using Window Function\n",
    "window_specs = Window.partitionBy(\"machine_id\",\"process_id\").orderBy(\"machine_id\",\"process_id\")\n",
    "activity.withColumn(\"start\",lag(\"timestamp\").over(window_specs)).filter(col(\"activity_type\")==\"end\")\\\n",
    "    .withColumn(\"time_taken\",col(\"timestamp\")-col(\"start\"))\\\n",
    "    .groupBy(\"machine_id\").agg(round(avg(\"time_taken\"),3).alias(\"processing_time\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6d0888-a492-495c-a3ec-a9f6e04a9c02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         1|          0.995|\n|         2|          1.456|\n|         0|          0.894|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using Inner Join\n",
    "a1 = activity.alias(\"a1\")\n",
    "a2 = activity.alias(\"a2\")\n",
    "a1.join(a2,(\\\n",
    "        (col(\"a1.machine_id\")==col(\"a2.machine_id\")) & \\\n",
    "        (col(\"a1.process_id\")==col(\"a2.process_id\")) & \\\n",
    "        (col(\"a1.activity_type\")!=col(\"a2.activity_type\")) &\\\n",
    "        (col(\"a1.activity_type\")==\"end\"))\\\n",
    "    ,\"inner\")\\\n",
    "    .select(a1.machine_id,col(\"a1.timestamp\").alias(\"endtime\"),col(\"a2.timestamp\").alias(\"starttime\"))\\\n",
    "    .withColumn(\"time\",col(\"endtime\")-col(\"starttime\"))\\\n",
    "    .groupBy(\"machine_id\").agg(round(avg(\"time\"),3).alias(\"processing_time\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e15110-ebc7-46b1-ac6d-402bf587d1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         1|          0.995|\n|         2|          1.456|\n|         0|          0.894|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "activity.createOrReplaceTempView(\"a\")\n",
    "spark.sql(\"\"\"select a1.machine_id , round(avg(a1.timestamp - a2.timestamp),3) as processing_time \n",
    "         from a a1 join a a2 \n",
    "         on a1.machine_id=a2.machine_id and \n",
    "            a1.process_id=a2.process_id and \n",
    "            a1.activity_type<>a2.activity_type and \n",
    "            a1.activity_type='end'\n",
    "        group by 1     \n",
    "         \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8239a8bd-4aa7-4b1d-9f99-343f563d0e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         1|          0.995|\n|         2|          1.456|\n|         0|          0.894|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          with cte as (\n",
    "                select machine_id,process_id,`timestamp` as end_time, lag(`timestamp`) over(partition by machine_id,process_id order by activity_type) as start_time from a order by machine_id,process_id )\n",
    "        select machine_id, abs(round(avg(end_time-start_time),3)) processing_time from cte where start_time is not null group by 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64350cad-7a5a-4a53-ba4a-8581170c64c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae66e6d-db7e-427b-a97e-38e59eb135bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1661. Average Time of Process per Machine",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
