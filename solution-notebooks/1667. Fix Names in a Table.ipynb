{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128c4a5f-8e82-41bb-b8ca-1641969ff24d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "from pyspark.sql.functions import col,udf\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5d7d40-e045-4e7c-9ae1-67067c794185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id| name|\n+-------+-----+\n|      1|aLice|\n|      2|  bOB|\n|      3| amIT|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\",IntegerType(),False),\n",
    "    StructField(\"name\",StringType(),False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1,\"aLice\"),\n",
    "    (2,\"bOB\"),\n",
    "    (3,\"amIT\")\n",
    "]\n",
    "names = spark.createDataFrame(data,schema)\n",
    "names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855a1b70-a971-49b5-b2f2-e4b71b36b84c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id| name|\n+-------+-----+\n|      1|Alice|\n|      2|  Bob|\n|      3| Amit|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.\n",
    "# Return the result table ordered by user_id.\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def title(df_col:str):\n",
    "    return df_col.title()\n",
    "\n",
    "names.withColumn(\"name\",title(\"name\")).orderBy(\"user_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68f5519-45cd-401a-9bd1-4d0dc53028f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id| name|\n+-------+-----+\n|      1|Alice|\n|      2|  Bob|\n|      3| Amit|\n+-------+-----+\n\n+-------+-----+\n|user_id| name|\n+-------+-----+\n|      1|Alice|\n|      2|  Bob|\n|      3| Amit|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "names.createOrReplaceTempView(\"names\")\n",
    "\n",
    "# Using udf in spark sql \n",
    "# Note: Dont specify any return type in udf as spark sql determines the output type during run time only\n",
    "@udf()\n",
    "def title(df_col:str):\n",
    "    return df_col.title()\n",
    "\n",
    "spark.sql(\"select user_id, title(name) name from names order by 1\").show()\n",
    "\n",
    "#Using concat, len, left and right functions\n",
    "spark.sql(\"select user_id, concat(upper(left(name,1)),lower(right(name,length(name)-1))) name from names order by 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49358e69-985d-4670-b1b6-08c81f4e0814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e801880d-cd80-4085-ade7-22964c26d82d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1667. Fix Names in a Table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
