{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc533b4-acbe-4d64-b506-a403d2bad810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType,StructType,StructField\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0096cc-7421-4363-bf89-517f5f4e6186",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+---------------+----------+\n|sales_id|name|salary|commission_rate| hire_date|\n+--------+----+------+---------------+----------+\n|       1|John|100000|              6|2006-04-01|\n|       2| Amy| 12000|              5|2010-05-01|\n|       3|Mark| 65000|             12|2008-12-25|\n|       4| Pam| 25000|             25|2005-01-01|\n|       5|Alex|  5000|             10|2007-02-03|\n+--------+----+------+---------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"sales_id\",IntegerType(),False),\n",
    "        StructField(\"name\",StringType(),False),\n",
    "        StructField(\"salary\",IntegerType(),True),\n",
    "        StructField(\"commission_rate\",IntegerType(),True),\n",
    "        StructField(\"hire_date\",DateType(),False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (1        , \"John\" , 100000 , 6 , datetime(2006,4,1) ), \n",
    "    (2        , \"Amy\"  , 12000  , 5 , datetime(2010,5,1) ), \n",
    "    (3        , \"Mark\" , 65000  , 12  , datetime(2008,12,25) ), \n",
    "    (4        , \"Pam\"  , 25000  , 25  , datetime(2005,1,1) ), \n",
    "    (5        , \"Alex\" , 5000   , 10 , datetime(2007,2,3) ), \n",
    "]\n",
    "\n",
    "salesPerson = spark.createDataFrame(data,schema)\n",
    "salesPerson.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d215c8e-f7c3-434d-8c32-1a4a97efeebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+\n|com_id|  name|    city|\n+------+------+--------+\n|     1|   RED|  Boston|\n|     2|ORANGE|New York|\n|     3|YELLOW|  Boston|\n|     4| GREEN|  Austin|\n+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"com_id\",IntegerType(),False),\n",
    "        StructField(\"name\",StringType(),False),\n",
    "        StructField(\"city\",StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (1      , \"RED\"    , \"Boston\"   ),\n",
    "    (2      , \"ORANGE\" , \"New York\" ),\n",
    "    (3      , \"YELLOW\" , \"Boston\"   ),\n",
    "    (4      , \"GREEN\"  , \"Austin\"   )\n",
    "]\n",
    "\n",
    "company = spark.createDataFrame(data,schema)\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919eb73f-0217-4975-8558-8cd512fb445b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+------+\n|order_id|order_date|com_id|sales_id|amount|\n+--------+----------+------+--------+------+\n|       1|2014-01-01|     3|       4| 10000|\n|       2|2014-02-01|     4|       5|  5000|\n|       3|2014-03-01|     1|       1| 50000|\n|       4|2014-04-01|     1|       4| 25000|\n+--------+----------+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"order_id\",IntegerType(),False),\n",
    "        StructField(\"order_date\",DateType(),False),\n",
    "        StructField(\"com_id\",IntegerType(),False),\n",
    "        StructField(\"sales_id\",IntegerType(),True),\n",
    "        StructField(\"amount\",IntegerType(),True)        \n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (1, datetime(2014,1,1), 3,4,10000 ),\n",
    "    (2, datetime(2014,2,1), 4,5,5000 ),\n",
    "    (3, datetime(2014,3,1), 1,1,50000 ),\n",
    "    (4, datetime(2014,4,1), 1,4,25000 )\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(data,schema)\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b4495c-b9fe-4c25-b590-7b7f68be4811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|sales_id|\n+--------+\n|       1|\n|       4|\n+--------+\n\n+----+\n|name|\n+----+\n| Amy|\n|Mark|\n|Alex|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the names of all the salespersons who did not have any orders related to the company with the name \"RED\". Return the result table in any order.\n",
    "\n",
    "sales_ids_to_skip = orders.alias('o').join(company.alias('c'),col(\"o.com_id\") == col(\"c.com_id\"),\"inner\").where(col(\"c.name\")==\"RED\").select(\"o.sales_id\")\n",
    "sales_ids_to_skip.show()\n",
    "\n",
    "salesPerson.alias(\"sp\").join(sales_ids_to_skip.alias(\"skip\"),col(\"sp.sales_id\")==col(\"skip.sales_id\"),\"left\").filter(col(\"skip.sales_id\").isNull()).select(\"sp.name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef93a05-307a-48b4-a5b5-cefb76eb8cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids to skip:  [1, 4]\n+----+\n|name|\n+----+\n| Amy|\n|Mark|\n|Alex|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# ids_to_skip = sales_ids_to_skip.select(\"sales_id\").rdd.flatMap(lambda x:x).collect()\n",
    "ids_to_skip = sales_ids_to_skip.rdd.map(lambda x: x.sales_id).collect()\n",
    "print(\"ids to skip: \",ids_to_skip)\n",
    "\n",
    "salesPerson.filter(~col(\"sales_id\").isin(ids_to_skip)).select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4052c780-f85d-45fd-a454-fb24a03efef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n| Amy|\n|Mark|\n|Alex|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "orders.createOrReplaceTempView(\"o\")\n",
    "salesPerson.createOrReplaceTempView(\"sp\")\n",
    "company.createOrReplaceTempView(\"c\")\n",
    "\n",
    "spark.sql(\"select name from sp where sp.sales_id not in (select o.sales_id from c  join o using(com_id) where c.name='RED')\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79416d61-a594-4712-b936-833be1c081fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "607. Sales Person",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
