{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73f31ad-cbcb-49ff-95bd-3062e6ccb593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"app\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc250ba2-b0e4-4f37-aa29-0aad0e883049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n|employee_id|    name|\n+-----------+--------+\n|          2|    Crew|\n|          4|   Haven|\n|          5|Kristian|\n+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"employee_id\",IntegerType(),False),\n",
    "    StructField(\"name\",StringType(),False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "( 2          , \"Crew\"     ),\n",
    "( 4          , \"Haven\"    ),\n",
    "( 5          , \"Kristian\" )\n",
    "]\n",
    "emp = spark.createDataFrame(data,schema)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfd8391-19b6-4e12-8e3d-f1165d9a7720",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|employee_id|salary|\n+-----------+------+\n|          5| 76071|\n|          1| 22517|\n|          4| 63539|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"employee_id\",IntegerType(),False),\n",
    "    StructField(\"salary\",IntegerType(),False)\n",
    "])\n",
    "data = [\n",
    "( 5           , 76071 ) ,\n",
    "( 1           , 22517 ) ,\n",
    "( 4           , 63539 ) \n",
    "]\n",
    "salaries = spark.createDataFrame(data,schema)\n",
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5758d27e-ceb2-4511-b8a8-0f7214fd88e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          2|\n|          1|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the IDs of all the employees with missing information. Return the result table ordered by employee_id in descending order.\n",
    "# The information of an employee is missing if:\n",
    "#     * The employee's name is missing, or\n",
    "#     * The employee's salary is missing.\n",
    "\n",
    "\n",
    "emp_left = emp.join(salaries,emp.employee_id==salaries.employee_id,'left_anti').select(\"employee_id\")\n",
    "salaries_left = salaries.join(emp,emp.employee_id==salaries.employee_id,'anti').select(\"employee_id\")\n",
    "\n",
    "emp_left.union(salaries_left).orderBy(\"employee_id\").sort(col(\"employee_id\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160fd0df-7372-4a29-923e-5d826c1ea0fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          2|\n|          1|\n+-----------+\n\n+-----------+\n|employee_id|\n+-----------+\n|          1|\n|          2|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.createOrReplaceTempView(\"emp\")\n",
    "salaries.createOrReplaceTempView(\"s\")\n",
    "\n",
    "# order by 1 desc works on full table created via union\n",
    "spark.sql(\"select employee_id from emp left anti join s using(employee_id) union select employee_id from s left anti join emp using (employee_id) order by 1 desc\").show()\n",
    "\n",
    "# Using left and right joins along with where condition to create left anti join\n",
    "spark.sql(\"\"\"\n",
    "            select e.employee_id from emp e left join s on e.employee_id=s.employee_id where s.employee_id is null\n",
    "                union\n",
    "            select s.employee_id from emp e right join s on e.employee_id=s.employee_id where e.employee_id is null\n",
    "            order by 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35adc68f-ae29-4504-a6ab-040a644c021e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d8f083e-ab05-4fb4-b503-8563d6efd801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1965. Employees With Missing Information",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
